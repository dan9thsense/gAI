{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-03 18:55:54,081] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3 # Learning rate, applicable to both nn, policy and model\n",
    "\n",
    "gamma = 0.99 # Discount factor for rewards\n",
    "\n",
    "decay_rate = 0.99 # Decay factor for RMSProp leaky sum of grad**2\n",
    "\n",
    "model_batch_size = 3 # Batch size used for training model nn\n",
    "policy_batch_size = 3 # Batch size used for training policy nn\n",
    "\n",
    "dimen = dimen = env.observation_space.shape[0] # Number of dimensions in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount(r, gamma=0.99, standardize=False):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [1, 0.99, 0.9801]\n",
    "    \"\"\"\n",
    "    discounted = np.array([val * (gamma ** i) for i, val in enumerate(r)])\n",
    "    if standardize:\n",
    "        discounted -= np.mean(discounted)\n",
    "        discounted /= np.std(discounted)\n",
    "    return discounted\n",
    "\n",
    "def step_model(sess, xs, action):\n",
    "    \"\"\" Uses our trained nn model to produce a new state given a previous state and action \"\"\"\n",
    "    # Last state\n",
    "    x = xs[-1].reshape(1,-1)\n",
    "    \n",
    "    # Append action\n",
    "    x = np.hstack([x, [[action]]])\n",
    "    \n",
    "    # Predict output\n",
    "    output_y = sess.run(predicted_state_m, feed_dict={input_x_m: x})\n",
    "    \n",
    "    # predicted_state_m == [state_0, state_1, state_2, state_3, reward, done]\n",
    "    output_next_state = output_y[:,:4]\n",
    "    output_reward = output_y[:,4]\n",
    "    output_done = output_y[:,5]\n",
    "    \n",
    "    # First and third env outputs are limited to +/- 2.4 and +/- 0.4\n",
    "    output_next_state[:,0] = np.clip(output_next_state[:,0],-2.4,2.4)\n",
    "    \n",
    "    output_next_state[:,2] = np.clip(output_next_state[:,2],-0.4,0.4)\n",
    "    \n",
    "    # Threshold for being done is likliehood being > 0.1\n",
    "    output_done = True if output_done > 0.01 or len(xs) > 500 else False\n",
    "    \n",
    "    return output_next_state, output_reward, output_done\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "num_hidden_m = 256\n",
    "dimen_m = dimen + 1 \n",
    "\n",
    "model_m = Sequential()\n",
    "model_m.add(Dense(num_hidden_m, input_dim=dimen_m, activation=\"relu\"))\n",
    "model_m.add(Dense(num_hidden_m, activation=\"relu\"))\n",
    "model_m.add(Dense(dimen + 1 + 1)) # output layer: next obs, reward, gameover\n",
    "\n",
    "model_m.compile(optimizer=Adam(lr=learning_rate), loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Policy network\n",
    "\n",
    "num_hidden_p = 256\n",
    "dimen_p = dimen\n",
    "\n",
    "model_p = Sequential()\n",
    "model_p.add(Dense(num_hidden_p, input_dim=dimen_p, activation=\"relu\"))\n",
    "model_p.add(Dense(2)) # Two outputs, one for action 0, one for action 1\n",
    "\n",
    "model_p.compile(optimizer=Adam(lr=learning_rate), loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 138 Training complete with total score of: 301.0\n"
     ]
    }
   ],
   "source": [
    "# Keep track our our rewards\n",
    "reward_sum = 0\n",
    "reward_total = []\n",
    "\n",
    "# Tracks the score on the real (non-simulated) environment to determine when to stop\n",
    "episode_count = 0\n",
    "num_episodes = 5000\n",
    "max_num_moves = 300\n",
    "\n",
    "# Setup array to keep track of observations, rewards and actions\n",
    "observations = np.empty(0).reshape(0,dimen)\n",
    "rewards = np.empty(0).reshape(0,1)\n",
    "actions = np.empty(0).reshape(0,1)\n",
    "policies = np.empty(0).reshape(0,2)\n",
    "\n",
    "draw_from_model = False\n",
    "train_the_model = False\n",
    "train_the_policy = True\n",
    "\n",
    "num_episode = 0\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    observation = observation.reshape(1,-1)\n",
    "    \n",
    "    # Determine the policy\n",
    "    policy = model_p.predict(observation)\n",
    "    policies = np.vstack([policies, policy])\n",
    "    \n",
    "    # Decide on an action based on the policy, allowing for some randomness\n",
    "    action = np.argmax(model_p.predict(observation)[0])\n",
    "\n",
    "    # Keep track of the observations and actions\n",
    "    observations = np.vstack([observations, observation])\n",
    "    actions = np.vstack([actions, action])\n",
    "    \n",
    "    # Determine next observation either from model or real environment\n",
    "    \n",
    "    if draw_from_model:\n",
    "        output = model_m.predict(np.hstack([observation, action]))\n",
    "        observation, reward, done = output[:4], output[4], output[5]\n",
    "    else:\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "    # Keep track of rewards\n",
    "    reward_sum += reward\n",
    "    rewards = np.vstack([rewards, reward])\n",
    "    \n",
    "    # If game is over or running long\n",
    "    if done or len(observations) > max_num_moves:\n",
    "        \n",
    "        # Keep track of how many real scenarios to determine average score from real environment \n",
    "        episode_count += 1\n",
    "        \n",
    "        # Keep track of rewards\n",
    "        reward_total.append(reward_sum)\n",
    "        \n",
    "        # Discount rewards\n",
    "        disc_rewards = discount(rewards, standardize=True)\n",
    "        \n",
    "        for idx, action, disc_reward in zip(range(len(actions)), actions, disc_rewards):\n",
    "            policies[idx, int(action[0])] = disc_reward\n",
    "        \n",
    "        num_episode += 1\n",
    "        \n",
    "        observation = env.reset()\n",
    "        \n",
    "        if train_the_policy:\n",
    "            model_p.train_on_batch(observations, policies)\n",
    "        \n",
    "        # Reset everything\n",
    "        observations = np.empty(0).reshape(0,dimen)\n",
    "        rewards = np.empty(0).reshape(0,1)\n",
    "        actions = np.empty(0).reshape(0,1)\n",
    "        policies = np.empty(0).reshape(0,2)\n",
    "        \n",
    "        # Print periodically\n",
    "        if (num_episode % (100 * policy_batch_size) == 0):\n",
    "            prob_random -= 0.1\n",
    "            prob_random = max(0.0, prob_random)\n",
    "            print(\"Episode {} rewards: {}\".format(\n",
    "                    num_episode, reward_sum/policy_batch_size))\n",
    "            \n",
    "\n",
    "        # If we our real score is good enough, quit\n",
    "        if episode_count > 0:\n",
    "            if (reward_sum/episode_count >= 300):\n",
    "                print(\"Episode {} Training complete with total score of: {}\".format(\n",
    "                        num_episode, reward_sum/episode_count))\n",
    "                break\n",
    "            episode_count = 0\n",
    "            reward_sum = 0\n",
    "\n",
    "        reward_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score: 301.0\n"
     ]
    }
   ],
   "source": [
    "# See our trained bot in action\n",
    "\n",
    "observation = env.reset()\n",
    "observation\n",
    "reward_sum = 0\n",
    "num_move = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    \n",
    "    x = np.reshape(observation, [1, dimen])\n",
    "    y = model_p.predict(x)\n",
    "    y = np.argmax(y[0])\n",
    "    observation, reward, done, _ = env.step(y)\n",
    "    reward_sum += reward\n",
    "    num_move += 1\n",
    "    \n",
    "    if done or num_move > max_num_moves:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
