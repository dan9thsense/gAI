{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL\n",
    "Re-write of code from [Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-28 05:22:38,595] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-2 # Learning rate, applicable to both nn, policy and model\n",
    "\n",
    "gamma = 0.99 # Discount factor for rewards\n",
    "\n",
    "decay_rate = 0.99 # Decay factor for RMSProp leaky sum of grad**2\n",
    "\n",
    "model_batch_size = 3 # Batch size used for training model nn\n",
    "policy_batch_size = 3 # Batch size used for training policy nn\n",
    "\n",
    "dimen = 4 # Number of dimensions in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount(r, gamma=0.99, standardize=False):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [1, 0.99, 0.9801]\n",
    "    \"\"\"\n",
    "    discounted = np.array([val * (gamma ** i) for i, val in enumerate(r)])\n",
    "    if standardize:\n",
    "        discounted -= np.mean(discounted)\n",
    "        discounted /= np.std(discounted)\n",
    "    return discounted\n",
    "\n",
    "def step_model(sess, xs, action):\n",
    "    \"\"\" Uses our trained nn model to produce a new state given a previous state and action \"\"\"\n",
    "    # Last state\n",
    "    x = xs[-1].reshape(1,-1)\n",
    "    \n",
    "    # Append action\n",
    "    x = np.hstack([x, [[action]]])\n",
    "    \n",
    "    # Predict output\n",
    "    output_y = sess.run(predicted_state_m, feed_dict={input_x_m: x})\n",
    "    \n",
    "    # predicted_state_m == [state_0, state_1, state_2, state_3, reward, done]\n",
    "    output_next_state = output_y[:,:4]\n",
    "    output_reward = output_y[:,4]\n",
    "    output_done = output_y[:,5]\n",
    "    \n",
    "    # First and third env outputs are limited to +/- 2.4 and +/- 0.4\n",
    "    output_next_state[:,0] = np.clip(output_next_state[:,0],-2.4,2.4)\n",
    "    \n",
    "    output_next_state[:,2] = np.clip(output_next_state[:,2],-0.4,0.4)\n",
    "    \n",
    "    # Threshold for being done is likliehood being > 0.01\n",
    "    output_done = True if output_done > 0.01 or len(xs) > 500 else False\n",
    "    \n",
    "    return output_next_state, output_reward, output_done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Neural Network\n",
    "The model neural network will be used to map the current environment and action to a future environment, reward and done condition (whether game is done). This will serve as an approximation to the actual environment, since there may be instances where training on an a real environment would be too costly or time-consuming. This neural network will learn to replicate the environment and will be useful in training a policy\n",
    "### Architecture\n",
    "The architecture will consist of two layers with 256 neurons each with relu activations. There will be three output layers, one for next observation, one for reward and one for a trigger whether the game is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "num_hidden_m = 256\n",
    "\n",
    "# Dimensions of the previous state plus 1 for the action\n",
    "dimen_m = dimen + 1\n",
    "\n",
    "# Placeholder for inputs\n",
    "input_x_m = tf.placeholder(tf.float32, [None, dimen_m])\n",
    "\n",
    "# First layer\n",
    "W1_m = tf.get_variable(\"W1_m\", shape=[dimen_m, num_hidden_m],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1_m = tf.Variable(tf.zeros([num_hidden_m]), name=\"B1M\")\n",
    "layer1_m = tf.nn.relu(tf.matmul(input_x_m, W1_m) + B1_m)\n",
    "\n",
    "# Second layer\n",
    "W2_m = tf.get_variable(\"W2_m\", shape=[num_hidden_m, num_hidden_m],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2_m = tf.Variable(tf.zeros([num_hidden_m]), name=\"B2_m\")\n",
    "layer2_m = tf.nn.relu(tf.matmul(layer1_m, W2_m) + B2_m)\n",
    "\n",
    "# Third (output) layers\n",
    "# Note that there are three separate output layers, \n",
    "# one for next observation, reward and whether the game is complete\n",
    "\n",
    "W_obs_m = tf.get_variable(\"W_obs_m\", shape=[num_hidden_m, 4],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B_obs_m = tf.Variable(tf.zeros([4]), name=\"B_obs_m\")\n",
    "\n",
    "W_reward_m = tf.get_variable(\"W_reward_m\", shape=[num_hidden_m, 1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B_reward_m = tf.Variable(tf.zeros([1]), name=\"B_reward_m\")\n",
    "\n",
    "W_done_m = tf.get_variable(\"W_done_m\", shape=[num_hidden_m,1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B_done_m = tf.Variable(tf.zeros([1]), name=\"B_done_m\")\n",
    "\n",
    "output_obs_m = tf.matmul(layer2_m, W_obs_m) + B_obs_m\n",
    "output_reward_m = tf.matmul(layer2_m, W_reward_m) + B_reward_m\n",
    "output_done_m = tf.sigmoid(tf.matmul(layer2_m, W_done_m) + B_done_m)\n",
    "\n",
    "# Placeholders for inputs used in training\n",
    "actual_obs_m = tf.placeholder(tf.float32, [None, dimen_m], name=\"actual_obs\")\n",
    "actual_reward_m = tf.placeholder(tf.float32, [None, 1], name=\"actual_reward\")\n",
    "actual_done_m = tf.placeholder(tf.float32, [None, 1], name=\"actual_done\")\n",
    "\n",
    "# Putting it all together\n",
    "predicted_state_m = tf.concat([output_obs_m, output_reward_m, output_done_m], axis=1)\n",
    "\n",
    "# Loss functions\n",
    "loss_obs_m = tf.square(actual_reward_m - output_reward_m)\n",
    "loss_reward_m = tf.square(actual_reward_m - output_reward_m)\n",
    "loss_done_m = -tf.log(actual_done_m * output_done_m + \n",
    "                (1 - actual_done_m) * (1 - output_done_m))\n",
    "\n",
    "# Model loss is simply the average loss of the three outputs\n",
    "loss_m = tf.reduce_max(loss_obs_m + loss_reward_m + loss_done_m)\n",
    "\n",
    "adam_m = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "update_m = adam_m.minimize(loss_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Neural Network\n",
    "The policy network will be used to determine policy. We will use a combination of a simulated environment from the model neural network we created and a real environment.\n",
    "### Architecture\n",
    "Our architecture will consist of one hidden layer with 10 neurons and an output layer used to determine the policy (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden_p = 10 # Number of hidden units in the nn used to determine policy\n",
    "\n",
    "input_x_p = tf.placeholder(tf.float32, [None, dimen], name=\"input_x\")\n",
    "\n",
    "# First layer\n",
    "W1_p = tf.get_variable(\"W1\", shape=[dimen,num_hidden_p], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1_p = tf.nn.relu(tf.matmul(input_x_p, W1_p))\n",
    "\n",
    "# Second layer\n",
    "W2_p = tf.get_variable(\"W2\", shape=[num_hidden_p, 1], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "output_p = tf.nn.sigmoid(tf.matmul(layer1_p, W2_p))\n",
    "\n",
    "# Placeholders for inputs used in training\n",
    "input_y_p = tf.placeholder(tf.float32, shape=[None, 1], name=\"input_y\")\n",
    "advantages_p = tf.placeholder(tf.float32, shape=[None,1], name=\"reward_signal\")\n",
    "\n",
    "# Loss function\n",
    "# Below is equivalent to: 0 if input_y_p == output_p else 1\n",
    "log_lik_p = tf.log(input_y_p * (input_y_p - output_p) + \n",
    "                 (1 - input_y_p) * (input_y_p + output_p))\n",
    "\n",
    "# We'll be trying to maximize log liklihood\n",
    "loss_p = -tf.reduce_mean(log_lik_p * advantages_p)\n",
    "\n",
    "# Gradients\n",
    "W1_grad_p = tf.placeholder(tf.float32,name=\"W1_grad\")\n",
    "W2_grad_p = tf.placeholder(tf.float32,name=\"W2_grad\")\n",
    "batch_grad_p = [W1_grad_p, W2_grad_p]\n",
    "trainable_vars_p = [W1_p, W2_p]\n",
    "grads_p = tf.gradients(loss_p, trainable_vars_p)\n",
    "\n",
    "# Optimizer\n",
    "adam_p = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Update function\n",
    "update_grads_p = adam_p.apply_gradients(zip(batch_grad_p, [W1_p, W2_p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: [[ 0.31172247  0.83957733  0.98781848  0.84498566]]\n",
      "action: 0\n",
      "output obs: [[-0.01895702 -0.04625274  0.09602192 -0.06129436]]\n",
      "ouput reward: [[-0.00686016]]\n",
      "output done: [[ 0.47249478]]\n",
      "output policy: [[ 0.45592654]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and test to see models are setup correctly\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "random_obs = np.random.random(size=[1, env.observation_space.shape[0]])\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "print(\"obs: {}\\naction: {}\\noutput obs: {}\\nouput reward: {}\\noutput done: {}\\noutput policy: {}\".format(\n",
    "        random_obs,\n",
    "        random_action,\n",
    "        sess.run(output_obs_m,feed_dict={input_x_m: np.hstack([random_obs, [[random_action]]])}),\n",
    "        sess.run(output_reward_m,feed_dict={input_x_m: np.hstack([random_obs, [[random_action]]])}),\n",
    "        sess.run(output_done_m,feed_dict={input_x_m: np.hstack([random_obs, [[random_action]]])}),\n",
    "        sess.run(output_p,feed_dict={input_x_p: random_obs})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training the model and policy will happen simultaneously. We will have to train the model first before we use it in training the policy, but afterward, the policy will switch off between training from the real environment and the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299 / 5000 Episode 300 last batch rewards: [ 28.]\n",
      "599 / 5000 Episode 600 last batch rewards: [ 142.66666667]\n",
      "899 / 5000 Episode 900 last batch rewards: [ 152.33333333]\n",
      "1199 / 5000 Episode 1200 last batch rewards: [ 219.33333333]\n",
      "1499 / 5000 Episode 1500 last batch rewards: [ 241.]\n",
      "1670 / 5000 Episode 1671 Training complete with total score of: [ 301.]\n"
     ]
    }
   ],
   "source": [
    "# Tracks the score on the real (non-simulated) environment to determine when to stop\n",
    "real_rewards = []\n",
    "num_episodes = 5000\n",
    "\n",
    "# Trigger used to decide whether we should train from model or from real environment\n",
    "train_from_model = False\n",
    "train_first_steps = 500\n",
    "\n",
    "# Setup array to keep track of observations, rewards and actions\n",
    "observations = np.empty(0).reshape(0,dimen)\n",
    "rewards = np.empty(0).reshape(0,1)\n",
    "actions = np.empty(0).reshape(0,1)\n",
    "\n",
    "# Gradients\n",
    "grads = np.array([np.zeros(var.get_shape().as_list()) for var in trainable_vars_p])\n",
    "\n",
    "num_episode = 0\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    observation = observation.reshape(1,-1)\n",
    "    \n",
    "    # Determine the policy\n",
    "    policy = sess.run(output_p, feed_dict={input_x_p: observation})\n",
    "    \n",
    "    # Decide on an action based on the policy, allowing for some randomness\n",
    "    action = 0 if policy > np.random.uniform() else 1\n",
    "\n",
    "    # Keep track of the observations and actions\n",
    "    observations = np.vstack([observations, observation])\n",
    "    actions = np.vstack([actions, action])\n",
    "    \n",
    "    # Determine next observation either from model or real environment\n",
    "    if train_from_model:\n",
    "        observation, reward, done = step_model(sess, observations, action)\n",
    "    else:\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        \n",
    "    # Keep track of rewards\n",
    "    rewards = np.vstack([rewards, reward])\n",
    "    dones = np.zeros(shape=(len(observations),1))\n",
    "    \n",
    "    # If game is over or running long\n",
    "    if done or len(observations) > 300:\n",
    "        print(\"\\r{} / {} \".format(num_episode, num_episodes),end=\"\")\n",
    "\n",
    "        # If we're not training our policy from our model, we'll train our model from the real env\n",
    "        if not train_from_model:\n",
    "             # Previous state and actions for training model\n",
    "            states = np.hstack([observations, actions])\n",
    "            prev_states = states[:-1,:]\n",
    "            next_states = states[1:, :]\n",
    "            next_rewards = rewards[1:, :]\n",
    "            next_dones = dones[1:, :]\n",
    "\n",
    "            feed_dict = {input_x_m: prev_states.astype(np.float32), \n",
    "                         actual_obs_m: next_states.astype(np.float32),\n",
    "                        actual_done_m: next_dones.astype(np.float32),\n",
    "                        actual_reward_m: next_rewards.astype(np.float32)}\n",
    "\n",
    "            loss, _ = sess.run([loss_m, update_m], feed_dict=feed_dict)\n",
    "            \n",
    "            real_rewards.append(sum(rewards))\n",
    "            \n",
    "        \n",
    "        # Discount rewards\n",
    "        disc_rewards = discount(rewards, standardize=True)\n",
    "        \n",
    "        # Add gradients to running batch\n",
    "        grads += sess.run(grads_p, feed_dict={input_x_p: observations,\n",
    "                                            input_y_p: actions,\n",
    "                                            advantages_p: disc_rewards})\n",
    "        \n",
    "        num_episode += 1\n",
    "        \n",
    "        observation = env.reset()\n",
    "\n",
    "        # Reset everything\n",
    "        observations = np.empty(0).reshape(0,dimen)\n",
    "        rewards = np.empty(0).reshape(0,1)\n",
    "        actions = np.empty(0).reshape(0,1)\n",
    "        \n",
    "        # Toggle between training from model and from real environment allowing sufficient time \n",
    "        # to train the model before its used for learning policy\n",
    "        if num_episode > train_first_steps:\n",
    "            train_from_model = not train_from_model \n",
    "\n",
    "        # If batch full\n",
    "        if num_episode % policy_batch_size == 0:\n",
    "            \n",
    "            # Update gradients\n",
    "            sess.run(update_grads_p, feed_dict={W1_grad_p: grads[0], W2_grad_p: grads[1]})\n",
    "            \n",
    "            # Reset gradients\n",
    "            grads = np.array([np.zeros(var.get_shape().as_list()) for var in trainable_vars_p])\n",
    "            \n",
    "            # Print periodically\n",
    "            if (num_episode % (100 * policy_batch_size) == 0):\n",
    "                print(\"Episode {} last batch rewards: {}\".format(\n",
    "                        num_episode, sum(real_rewards[-policy_batch_size:])/policy_batch_size))\n",
    "            \n",
    "            # If our real score is good enough, quit\n",
    "            if (sum(real_rewards[-10:]) / 10. >= 300):\n",
    "                print(\"Episode {} Training complete with total score of: {}\".format(\n",
    "                        num_episode, sum(real_rewards[-policy_batch_size:])/policy_batch_size))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score: 2376.0\n"
     ]
    }
   ],
   "source": [
    "# See our trained bot in action\n",
    "\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "\n",
    "model_losses = []\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    \n",
    "    observation = np.reshape(observation, [1, -1])\n",
    "    policy = sess.run(output_p, feed_dict={input_x_p: observation})\n",
    "    action = 0 if policy > 0.5 else 1\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
